{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "\n",
    "Run the cells below for the basic setup of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('No colab environment, assuming local setup.')\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # turorials folder, e.g. 'alphafold-decoded/tutorials'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "    %pip install py3dmol\n",
    "    %pip install modelcif\n",
    "\n",
    "    print('Connected COLAB to Google Drive.')\n",
    "\n",
    "import os\n",
    "    \n",
    "base_folder = 'model'\n",
    "control_folder = f'{base_folder}/control_values'\n",
    "\n",
    "assert os.path.isdir(control_folder), 'Folder \"control_values\" not found, make sure that FOLDERNAME is set correctly.' if IN_COLAB else 'Folder \"control_values\" not found, make sure that your root folder is set correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to download the Openfold Weights\n",
    "import subprocess\n",
    "\n",
    "if not os.path.isdir('model/openfold_params') or not os.listdir('model/openfold_params'):\n",
    "    print('Download Openfold weights...')\n",
    "    %pip install awscli\n",
    "    subprocess.call(['bash', 'model/download_openfold_params.sh', 'model'])\n",
    "else:\n",
    "    print('Weights folder already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We are ready. All the parts of Alphafold are implemented, and all that's left to do is to stitch them together into the full model. \n",
    "\n",
    "It is described in Algorithm 2 in the paper. Note that we omit the template stack, and we don't do ensembling. Therefore, you can omit the lines 3, 4, 19, 20, and 7-13.\n",
    "\n",
    "Go to `model.py` and implement the initialization and the forward pass. After that, check your code by running the following cell. You might have to fix some datatype missmatches that fell through earlier checks. If they come up, go back to the methods and make sure that any tensors you create in them are of the right dtype (you can grab the dtype from one of the arguments of the method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import Model\n",
    "from model.control_values.model_checks import c_m, c_z, c_e, f_e, tf_dim, c_s, num_blocks_evoformer, num_blocks_extra_msa\n",
    "from model.control_values.model_checks import test_module_shape, test_module_method\n",
    "\n",
    "model = Model(c_m, c_z, c_e, f_e, tf_dim, c_s, num_blocks_extra_msa, num_blocks_evoformer)\n",
    "\n",
    "test_module_shape(model, 'model', control_folder)\n",
    "\n",
    "def test_method(*args):\n",
    "    outputs = model(*args)\n",
    "    return outputs['final_positions'], outputs['position_mask'], outputs['angles'], outputs['frames']\n",
    "\n",
    "\n",
    "test_module_method(model, 'model', 'batch', ('final_positions', 'position_mask', 'angles', 'frames'), control_folder, test_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Real Model\n",
    "\n",
    "Our model passed the check for the toy input. Now it's time to load the model in real size, with real weights, and do a real evaluation pass. For this, it's much faster to use the GPU. If you are in Colab, make sure select a GPU under \"Runtime -> Change runtime type\" if you haven't done so before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('Compatible GPU available.')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print('MPS (Metal Performance Shaders) available.')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('No compatible GPU, fallback to CPU.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the weights from Openfold for our model, as it is also written in PyTorch. However, we made some tweaks to the data without changing the actual weights (like renaming a few modules, swapping the angles so that they are predicted as (cos(phi), sin(phi)) like it's said in the paper, not (sin(phi), cos(phi))). You can look at the tweaks in `utils.py` in the method `load_openfold_weights`. \n",
    "\n",
    "If you named all your modules according to the descriptions, the following cell should run cleanly and tell you that all keys in the state dictionary matched successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import load_openfold_weights\n",
    "model = Model()\n",
    "openfold_weights = load_openfold_weights('model/openfold_params/finetuning_2.pt')\n",
    "\n",
    "model.load_state_dict(openfold_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step in the forward pass, we'll call `create_features_from_a3m` four times in a row, to get four different input batches (different because of the random selection of cluster centers) four the four different cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction.feature_extraction import create_features_from_a3m\n",
    "\n",
    "batch = None\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Compute four single batches with create_features_from_a3m.       #\n",
    "#   Then, stack all the tensors in the list to get the complete batch.   #\n",
    "##########################################################################\n",
    "\n",
    "single_cycle_batches = []\n",
    "for i in range(4):\n",
    "    single_cycle_batch = create_features_from_a3m('feature_extraction/alignment_tautomerase.a3m')\n",
    "    single_cycle_batches.append(single_cycle_batch)\n",
    "\n",
    "batch = {\n",
    "    key: torch.stack([single_batch[key] for single_batch in single_cycle_batches], dim=-1)\n",
    "    for key in single_cycle_batches[0].keys()\n",
    "}\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "expected_shapes = {\n",
    "    'msa_feat': (512, 59, 49, 4),\n",
    "    'extra_msa_feat': (5120, 59, 25, 4),\n",
    "    'target_feat': (59, 21, 4),\n",
    "    'residue_index': (59, 4),\n",
    "}\n",
    "\n",
    "shapes = { key: value.shape for key, value in batch.items() }\n",
    "assert set(expected_shapes.keys()) == set(shapes.keys())\n",
    "for key, shape in shapes.items():\n",
    "    assert expected_shapes[key] == shape, f'Shape mismatch for {key}: {shape} vs {expected_shapes[key]}'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a faster forward pass, we'll map the model and the inputs to the GPU, if a compatible one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for key, value in batch.items():\n",
    "    batch[key] = value.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for the forward pass. If you weren't careful with mapping all the tensors you created within the methods to the correct device (which you can grab from any of the input tensors of the methods), you'll have to do some debugging before this runs through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a little wrapper around the modelcif python library, to create an mmcif file from our atom positions. If you are interested, you can inspect it in `utils.py`. The following cell uses this method to create `prediction.mmcif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import to_modelcif\n",
    "from geometry.residue_constants import restypes\n",
    "\n",
    "atom_positions = outputs['final_positions'][..., -1]\n",
    "atom_mask = outputs['position_mask'][..., -1]\n",
    "seq_inds = batch['target_feat'].cpu()[..., -1].argmax(dim=-1).numpy()\n",
    "seq = ''.join([restypes[ind] for ind in seq_inds])\n",
    "\n",
    "cif_str = to_modelcif(atom_positions, atom_mask, seq)\n",
    "\n",
    "with open('model/prediction.mmcif', 'w') as f:\n",
    "    f.write(cif_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The py3Dmol library allows us to visualize our prediction in Jupyter Notebooks. You can uncomment the second `view = ...` line and comment the first one out, to display the protein prediction alongside the crystal structure from the PDB (if you do so, the little sticks in the background are the second part of the dimer from the PDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "view = py3Dmol.view()\n",
    "view = py3Dmol.view(query='2OP8')\n",
    "\n",
    "with open('model/prediction.mmcif', 'r') as f:\n",
    "    data = f.read()\n",
    "view.addModel(data, 'mmcif')\n",
    "view.addModel('2OP8')\n",
    "view.setStyle({'chain': 'A'}, {'cartoon': {'color':'spectrum'}})\n",
    "view.zoomTo()\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working in Colab, you can run the following cell to download the resulting mmcif file. Open the file in a protein view like ChimeraX on your local machine. Load the crystal structure as well. You can use the 'Tools -> Structure Analysis -> Matchmaker' to align your prediction with the template. The root mean square deviation (the error) for the alignment should be really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're in Colab, you can run this cell to download your prediction\n",
    "if IN_COLAB:\n",
    "    from google.colab import files # type: ignore\n",
    "    files.download('model/prediction.mmcif')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
